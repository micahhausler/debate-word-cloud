{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pip install wordcloud nltk matplotlib pillow ipython[notebook]\n",
    "# >>> nltk.download()\n",
    "#\n",
    "import os\n",
    "import json\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://www.nytimes.com/2015/11/11/us/politics/transcript-republican-presidential-debate.html'\n",
    "response = requests.get(url)\n",
    "raw = BeautifulSoup(response.text, \"html.parser\")\n",
    "paragraphs = raw.find_all('p', {'class': \"story-body-text story-content\", 'itemprop': \"articleBody\"})\n",
    "\n",
    "speakers = [\n",
    "    'TRUMP',\n",
    "    'CARSON',\n",
    "    'RUBIO',\n",
    "    'CRUZ',\n",
    "    'BUSH',\n",
    "    'FIORINA',\n",
    "    'KASICH',\n",
    "    'PAUL',\n",
    "    'CAVUTO',\n",
    "    'BAKER',\n",
    "    'BARTIROMO',\n",
    "    '(UNKNOWN)',\n",
    "]\n",
    "\n",
    "IGNORE_PARAGRAPHS = [\n",
    "    '(APPLAUSE)',\n",
    "    '(LAUGHTER)',\n",
    "    '(CHEERING)',\n",
    "    '(CHEERING AND APPLAUSE)',\n",
    "    '(AUDIENCE REACTION)',\n",
    "    '(BOOING)',\n",
    "    '(CROSSTALK)',\n",
    "    '(COMMERCIAL BREAK)',\n",
    "    '(BELL RINGING)',\n",
    "    '(AUDIO GAP)',\n",
    "    '(MUSIC)',\n",
    "    'MORE',\n",
    "]\n",
    "\n",
    "pgs = defaultdict(list)\n",
    "\n",
    "last_speaker = None\n",
    "\n",
    "for p in paragraphs:\n",
    "    if p.text in IGNORE_PARAGRAPHS:\n",
    "        continue\n",
    "    \n",
    "    p_has_speaker = False\n",
    "    for speaker in speakers:\n",
    "        if p.text.startswith('{}:'.format(speaker)):\n",
    "            last_speaker = speaker\n",
    "            p_has_speaker = True \n",
    "            pgs[speaker].append(p)\n",
    "            continue\n",
    "    if not p_has_speaker:\n",
    "        pgs[last_speaker].append(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_print(common_words):\n",
    "    wps = ['  {word}: {count}'.format(word=wp[0], count=wp[1]) for wp in common_words]\n",
    "    return ', '.join(wps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_cloud(candidate):\n",
    "    SPEAKER = candidate\n",
    "    all_text = ' '.join([p.text for p in pgs[SPEAKER]])\n",
    "    all_text = all_text.replace(SPEAKER, '').replace('Dodd-Frank', 'DoddFrank').replace('Middle East', 'MiddleEast')\n",
    "    all_text = all_text.replace(\"don't\", 'dont')\n",
    "    \n",
    "    tokens = nltk.wordpunct_tokenize(all_text)\n",
    "    tagged_tokens = nltk.pos_tag([token.lower() for token in tokens])\n",
    "    fd = nltk.FreqDist(tagged_tokens)\n",
    "\n",
    "    interesting_classes = ['NN', 'NNP', 'NNS', 'VB', 'VBP', 'JJ', 'JJS', 'JJR', 'CD', 'NNPS', 'VBG']\n",
    "    stop_words = [tok[0][0] for tok in fd.most_common(10000)  if  tok[0][1] not in interesting_classes]\n",
    "    stop_words.extend(['be', 'u', 'going', 'have', 'are'])\n",
    "\n",
    "    stop_words.extend(['have', '’', 'don', 're', '—', 't', 'do'])\n",
    "    stop_words.extend(['1', '2', '5', 'm', 'i', 've', 's', '“', 'll'])\n",
    "    \n",
    "    most_common_words = [(tok[0][0], tok[1]) for tok in fd.most_common(10000)  if  tok[0][0] not in stop_words]\n",
    "    most_common_words = most_common_words[:8]\n",
    "\n",
    "    wordcloud = WordCloud(stopwords=stop_words, width=1600, height=800).generate(all_text)\n",
    "    plt.figure(figsize=(24,12))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.suptitle('{0}\\'s Word Cloud'.format(SPEAKER.title()), fontsize=24, fontweight='bold')\n",
    "    plt.title(pretty_print(most_common_words), fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    home_dir = os.path.expanduser('~')\n",
    "    output_file = os.path.join(\n",
    "        home_dir,\n",
    "        'Desktop',\n",
    "        'wordcloud',\n",
    "        '{0}_word_cloud.png'.format(SPEAKER.lower())\n",
    "    )\n",
    "\n",
    "    plt.savefig(output_file, bbox_inches='tight')\n",
    "\n",
    "for candidate in speakers[:8]:\n",
    "    generate_cloud(candidate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
